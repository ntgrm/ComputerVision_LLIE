# Low-light image enhancement (LLIE)

## Описание задачи
• Задача — улучшение качества изображений при слабом освещении  
• Язык программирования Python  
• Ограничений по использованию библиотек и сторонних функций нет  

## Датасет
Общий загружаемый датасет 'general_dataset.zip' содержит в себе:  
• исходный датасет 'train_val', которые далее разбивается на обучающую и валидационную выборку в соотношении примерно 70/30 (он содержит 739 пар изображений: 500 будет в обучающей выборке, 239 — в валидационной)  
• тестовый датасет 'test'  

Каждый из этих датасетов ('train_val' и 'test') состоит из двух папок:  
• в папке 'low' содержатся изображения при слабом освещении  
• в папке 'high' — соответствующие им эталонные изображения при хорошем освещении  

## Метрики качества
Для оценки качества используются следующие метрики:  
• PSNR — Пиковое отношение сигнала к шуму  
• SSIM — Индекс структурного сходства  
• LPIPS — Learned Perceptual Image Patch Similarity  

## Модель
Для выполнения данной задачи используется модель MIRNet, она имеет полностью сверточную архитектуру и не требует на вход какого-либо определенного размера изображений.  
Её особенности следующие:  
• Она извлекает признаки в разных пространственных масштабах, сохраняя при этом исходные признаки с высоким разрешением для сохранения точных пространственных деталей.  
• Имеет регулярно повторяющийся механизм обмена информацией, в котором функции ветвей с несколькими разрешениями постепенно объединяются для улучшения обучения представлению.  
• Реализует новый подход к объединению многомасштабных объектов с использованием сети выборочного ядра, которая динамически объединяет переменные рецептивные поля и точно сохраняет исходную информацию об объектах при каждом пространственном разрешении.  
• Имеет рекурсивный дизайн, который постепенно разбивает входной сигнал, чтобы упростить общий процесс обучения, и позволяет строить очень глубокие сети.  

## Обучение модели
• Для обучения модели MIRNet в качестве loss-функции используется Charbonnier Loss, а в качестве оптимизатора — Adam со скоростью обучения (learning rate) = 1e-4. Количество эпох = 50.  
• Для улучшения работы модели скорость обучения (learning rate) будет уменьшаться, когда мы выходим на плато. Это осуществляется с помощью обратного вызова (callback), который следит за метрикой PSNR, и если за 5 эпох не происходит улучшения, то скорость обучения уменьшается.  
• Отслеживается зависимость изменения функции потерь, а также метрик PSNR и SSIM от номера эпохи для обучающей и валидационной выборки, и в результате выводятся соответствующие графики.

![Loss](https://github.com/ntgrm/ComputerVision_LLIE/blob/main/results/1_Loss.png)
![PSNR](https://github.com/ntgrm/ComputerVision_LLIE/blob/main/results/2_PSNR.png)
![SSIM](https://github.com/ntgrm/ComputerVision_LLIE/blob/main/results/3_SSIM.png)

## Результаты
• Ссылка на решение в Google Colab:  
https://colab.research.google.com/drive/1Ogpzuazo-zKZp99o5TZHWhCGdOzfsIx3?usp=sharing

• В данной реализации на обработку одного изображения уходит 1.44 с  
• Полученные значения метрик качества:

|     PSNR     |      SSIM      |     LPIPS     |
|     :---:    |     :---:      |     :---:     |
| 15.487       | 0.712          | 0.270         |

• Примеры исходных и обработанных с помощью MIRNet изображений:

![24_25](https://github.com/ntgrm/ComputerVision_LLIE/blob/main/results/plot_results_24_25.JPG)
![46_47](https://github.com/ntgrm/ComputerVision_LLIE/blob/main/results/plot_results_46_47.JPG)
![50_51](https://github.com/ntgrm/ComputerVision_LLIE/blob/main/results/plot_results_50_51.JPG)
![63_64](https://github.com/ntgrm/ComputerVision_LLIE/blob/main/results/plot_results_63_64.JPG)
